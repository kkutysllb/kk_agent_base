{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义调用本地大模型方法\n",
    "1. **类属性定义**:\n",
    "   - `max_token`: 定义了模型可以处理的最大令牌数。\n",
    "   - `do_sample`: 指定是否在生成文本时采用采样策略。\n",
    "   - `temperature`: 控制生成文本的随机性，较高的值会产生更多随机性。\n",
    "   - `top_p`: 一种替代`temperature`的采样策略，这里设置为0.0，意味着不使用。\n",
    "   - `tokenizer`: 分词器，用于将文本转换为模型可以理解的令牌。\n",
    "   - `model`: 存储加载的模型对象。\n",
    "   - `history`: 存储对话历史。\n",
    "2. **构造函数**:\n",
    "   - `__init__`: 构造函数初始化了父类的属性。\n",
    "3. **属性方法**:\n",
    "   - `_llm_type`: 返回模型的类型，即`ChatGLM3`。\n",
    "4. **加载模型的方法**:\n",
    "   - `load_model`: 此方法用于加载模型和分词器。它首先尝试从指定的路径加载分词器，然后加载模型，并将模型设置为评估模式。这里的模型和分词器是从Hugging Face的`transformers`库中加载的。\n",
    "5. **调用方法**:\n",
    "   - `_call`: 一个内部方法，用于调用模型。它被设计为可以被子类覆盖。\n",
    "   - `invoke`: 这个方法使用模型进行聊天。它接受一个提示和一个历史记录，并返回模型的回复和更新后的历史记录。这里使用了模型的方法`chat`来生成回复，并设置了采样、最大长度和温度等参数。\n",
    "6. **流式方法**:\n",
    "   - `stream`: 这个方法允许模型逐步返回回复，而不是一次性返回所有内容。这对于长回复或者需要实时显示回复的场景很有用。它通过模型的方法`stream_chat`实现，并逐块返回回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/libing/miniconda3/envs/kk_agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/libing/miniconda3/envs/kk_agent/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "class kk_ChatGLM3(LLM):\n",
    "    max_token: int = 128 * 1024\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.5\n",
    "    top_p: float = 0.0\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history: list = []\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_model(self, model_path: str):\n",
    "        # 加载分词器\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "        # 加载模型\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device_map=\"mps\", use_safetensors=False)\n",
    "        model = model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def _call(self, prompt: str, config: dict = {}, history: list = []) -> str:\n",
    "        return self.invoke(prompt, config, history)\n",
    "    \n",
    "    def invoke(self, prompt: str, config: dict = {}, history: list = []) -> str:\n",
    "        # 处理 ChatPromptValue\n",
    "        if hasattr(prompt, 'to_messages'):\n",
    "            # 如果是 ChatPromptValue，提取消息内容\n",
    "            messages = prompt.to_messages()\n",
    "            prompt = messages[0].content\n",
    "        elif not isinstance(prompt, str):\n",
    "            # 尝试转换为字符串\n",
    "            prompt = str(prompt)\n",
    "\n",
    "        if not isinstance(history, list):\n",
    "            history = []\n",
    "        \n",
    "        if not isinstance(config, dict):\n",
    "            config = {}\n",
    "\n",
    "        response, history = self.model.chat(\n",
    "            self.tokenizer, \n",
    "            prompt, \n",
    "            history=history,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            max_length=self.max_token,\n",
    "        )\n",
    "        self.history = history\n",
    "        return AIMessage(content=response)\n",
    "\n",
    "    def stream(self, prompt: str, config: dict = {}, history: list = []) -> str:\n",
    "        # 处理 ChatPromptValue\n",
    "        if hasattr(prompt, 'to_messages'):\n",
    "            # 如果是 ChatPromptValue，提取消息内容\n",
    "            messages = prompt.to_messages()\n",
    "            prompt = messages[0].content\n",
    "        elif not isinstance(prompt, str):\n",
    "            # 尝试转换为字符串\n",
    "            prompt = str(prompt)\n",
    "        \n",
    "        if not isinstance(history, list):\n",
    "            history = []\n",
    "        \n",
    "        if not isinstance(config, dict):\n",
    "            config = {}\n",
    "        preResponese = \"\"\n",
    "        for response, _ in self.model.stream_chat(self.tokenizer, prompt):\n",
    "            if preResponese == \"\":\n",
    "                result = response\n",
    "            else:\n",
    "                result = response[len(preResponese):]\n",
    "            preResponese = response\n",
    "            yield result\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"kkChatGLM3\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting unk_token is not supported, use the default one.\n",
      "/Users/libing/miniconda3/envs/kk_agent/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/libing/miniconda3/envs/kk_agent/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "llm = kk_ChatGLM3()\n",
    "model_path = \"/Users/libing/kk_LLMs/chatglm3-6b\"\n",
    "llm.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='中国的首都是北京。', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用call方法\n",
    "llm.invoke(\"中国的首都是哪里？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天是国庆节,\n",
      "75周年庆典,\n",
      "举国欢腾,\n",
      "庆祝这个伟大的日子。\n",
      "\n",
      "75年前,\n",
      "祖国遭受着入侵,\n",
      "人民遭受着苦难,\n",
      "但有一群人,\n",
      "不畏强敌,\n",
      "不畏艰险,\n",
      "用自己的汗水和智慧,\n",
      "为国家赢得了独立和自由。\n",
      "\n",
      "75年后,\n",
      "祖国发生了巨变,\n",
      "经济繁荣,\n",
      "文化繁荣,\n",
      "人民幸福,\n",
      "国家强大。\n",
      "\n",
      "今天,\n",
      "我们欢庆这个伟大的日子,\n",
      "祝福我们的国家,\n",
      "祝福我们的人民,\n",
      "愿我们的国家永远繁荣昌盛,\n",
      "永远强大。"
     ]
    }
   ],
   "source": [
    "# 流式输出\n",
    "for respone in llm.stream(\"写一首关于75周年国庆节的诗歌\"):\n",
    "    print(respone, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题：【股市传奇】米晓斌：从新手到A股投资大师，只需一个信念\n",
      "\n",
      "导语：在繁杂的股海中，每一位投资者都在寻找属于自己的投资之道。而米晓斌，一位普通的中国散户，凭借着自己的信念和坚持，成为了一位传奇的A股投资者。今天，我们就来听听他的投资故事。\n",
      "\n",
      "正文：\n",
      "\n",
      "米晓斌，一个普通的上班族，在朋友的推荐下，开始了他的股市之旅。起初，他对股市知之甚少，甚至 considered 自己是一个完全的股市新手。然而，他并没有因此而气馁，反而下定决心，要在这个市场里大显身手。\n",
      "\n",
      "米晓斌开始通过阅读各种投资书籍和参加投资讲座来提升自己的投资水平。在不断学习和实践的过程中，他逐渐发现，要想在股市中取得成功，关键在于坚持自己的信念，并根据市场情况做出灵活的调整。\n",
      "\n",
      "有一次，米晓斌看中了一支潜力股，他坚定地相信这只股票的未来前景。尽管朋友们都认为他过于乐观，但他仍然坚定地持有这只股票。最终，这支股票涨了数倍，让米晓斌赚取了可观的利润。\n",
      "\n",
      "从那以后，米晓斌更加坚定自己的信念，并不断总结自己的投资经验。他认为，只有坚持自己的信念，并根据市场情况做出灵活的调整，才能在股市中取得成功。\n",
      "\n",
      "最后，米晓斌想送给正在股市中奋斗的朋友们一句名言：“信念是成功的关键。”只要坚定自己的信念，并根据市场情况做出灵活的调整，相信每位投资者都能在股市中取得成功。"
     ]
    }
   ],
   "source": [
    "# 链式输出\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"请根据提示词主题写一篇小红书的营销短文:  {topic}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"topic\": \"讲一个米晓斌投资a股的故事\"}):\n",
    "    print(chunk, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
